
<!DOCTYPE HTML5>

<html>

<!-- header and metadata -->

<head>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-149872355-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

        gtag('config', 'UA-149872355-1');
    </script>

    <title>Quzhe Huang</title>

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="Quzhe Huang&#39;s Homepage" http-equiv="Content-Type" content="Quzhe Huang&#39;s Homepage">
    <meta name="descripction" content="Quzhe Huang is a Ph.D. student at Peking University.">
    <meta name="keywords" content="Quzhe Huang, Peking University, Wangxuan Institute, Ph.D. student, Computer Science">
    <meta name="msvalidate.01" content="1B5B5E55BA1F102E42758748AA297708" />
    <link rel="icon" type="image/png" href="./figs/pku.png">
    <link rel="stylesheet" type="text/css" href="./stylesheet.css">


    <script src="js/functions.js"></script>

</head>

<body data-gr-c-s-loaded="true">
<table width="840" border="0" align="center" cellspacing="0" cellpadding="20"><tbody><tr><td>

<!-- title -->

<p align="center">
    <pageheading>Quzhe Huang</pageheading>
    <br>
    <b>Email</b>:<font id="email" style="display:inline;">huangquzhe [AT] pku [DOT] edu [DOT] cn</font>
</p>

<!-- avatar and bio -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody><tr>
    <td width="32%" valign="top">
        <img src="./figs/Me.jpeg" width="100%" style="border-radius:20px">
        <p align="center">

        <!-- | <a href="./figs/CV_hqz.pdf">CV</a> | <a href="https://scholar.google.com/citations?user=OGITFL4AAAAJ&hl=en">Google Scholar</a> | <a href="https://github.com/AndrewZhe">Github</a> |
        </p> -->
    </td>

    <td width="68%" valign="center" align="justify">
        <p> I am a fifth-year Ph.D. student at <a href="https://www.icst.pku.edu.cn/english/about_us/introduction/index.htm"> Wangxuan Institute of Computer Technology</a>, <a href="https://english.pku.edu.cn/"> Peking University</a>, advised by Prof. <a href="https://sites.google.com/site/ysfeng/home"> Yansong Feng </a> and Prof. <a href="https://www.wict.pku.edu.cn/zhaodongyan/en/"> Dongyan Zhao </a>.
        <p>
            I am expected to graduate in July and looking for postdoc or other job opportunity now!
        </p>
        <p>

            | <a href="./figs/CV_hqz.pdf">CV</a> | <a href="https://scholar.google.com/citations?user=OGITFL4AAAAJ&hl=en">Google Scholar</a> | <a href="https://github.com/AndrewZhe">Github</a> |
            </p>
    </td>
</tr></tbody></table>

<!-- news -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
    <tbody><tr><td><sectionheading>&nbsp;&nbsp;Research Interests</sectionheading></td></tr></tbody>
  </table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody><tr>  
    <!-- <td width="2%" valign="top" align="center"></td> -->
    <td width="5%" valign="top" align="center">

    </td>

    <td width="90%" valign="top">
        <p>My research interest lies in designing efficient LLM and adapting LLM to different modality and domains, with the goal of building practical AI systems:</p>
        <ul>
            <li> <b>Efficient LLM</b>: build small but competitive long-context LLM from scratch(<a href="./figs/A_Transparent_Binlingual_LLM.pdf">Tech Report</a>) and reduce the cost of both training and inference by dynamically allocating resources(<a href="https://arxiv.org/pdf/2403.07652">ACL 2024</a>). </li>

            <li> <b>Domain-Specific LLM</b>: adapt LLM to specific domain by integrating domain knowledge(<a href="https://arxiv.org/pdf/2305.15062">Tech Report</a>) and evaluate whether models reason as domain experts(<a href="https://arxiv.org/pdf/2210.17108">EMNLP 2022</a>). </li>
            
            <li> <b>Multimodal LLM</b>: design unified image-video-language pre-training framework(<a href="https://arxiv.org/pdf/2309.04669">ICLR 2024</a>, <a href="https://arxiv.org/pdf/2402.03161">ICML 2024</a>) and explore the ability of encoding global information(<a href="https://arxiv.org/pdf/2402.17304">COLING 2024</a>).</li>
        </ul>
        
        <p> I am also interested in long context reasoning(<a href="https://openreview.net/pdf?id=Cjp6YKVeAa">ICLR 2024</a>), document-level information extraction(<a href="https://aclanthology.org/2021.acl-short.126.pdf">ACL 2021_1</a> , <a href="https://aclanthology.org/2021.acl-long.433.pdf">ACL 2021_2</a>, <a href="https://arxiv.org/pdf/2204.07980.pdf">ACL 2022</a>, <a href="https://arxiv.org/pdf/2305.17607.pdf">ACL 2023</a>, <a href="https://arxiv.org/pdf/2310.16358">EMNLP 2023</a>), and low-resource languages (<a href="https://arxiv.org/pdf/2311.08348.pdf"> ACL 2024</a>) </p>
    </td>
</tr></tbody></table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
    <tbody><tr><td><sectionheading>&nbsp;&nbsp;News</sectionheading></td></tr></tbody>
  </table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"> <tbody><tr>
    <td width="5%" valign="top" align="center">

    </td>

        <td width="90%" valign="top">
        
            <ul>
                <li> <b>[June. 2024]</b> I am expected to graduate in July and looking for postdoc or other job opportunity now!</li>
                <li> <b>[May. 2024]</b> Our two papers, which are about <a href="https://arxiv.org/pdf/2403.07652">efficient MoE</a> and <a href="https://arxiv.org/pdf/2311.08348.pdf"> minority languages in China</a>, are accepted in ACL 2024!</li>
                <li> <b>[May. 2024]</b> Our work about unified video-language pre-training, <a href="https://video-lavit.github.io/">Video-LaVIT</a>, is accepted in ICML 2024! </li>
                <li> <b>[Apr. 2024]</b> Our work about explore the multimodal LLM's ability of encoding global information is accepted in COLING 2024!</li>

                <!-- <li> <b>[Dec. 2023]</b> We have succeeded to train a LLM from scratch! It could process text with up to 128K. Coming soon </li>
                <li> <b>[Oct. 2023]</b> We have built a unified language-vision pretraining framework, and our model  <a href="https://github.com/jy0205/LaVIT"> LaVIT </a> is available now!  </li>
                <li> <b>[Oct. 2023]</b> Three papers are accepted in EMNLP 2023 ! </li>
                <li> <b>[May. 2023]</b> We are building a domain-specific LLMs, <a href="https://github.com/AndrewZhe/lawyer-llama"> Lawyer LLaMA </a> !  </li>
                <li> <b>[May. 2023]</b> Our paper of proposing a unified framework for event temporal RE is accepted in ACL 2023 </li> -->
                <!-- <a href="javascript:toggleblock('old_news')">---- show more ----</a>
                <li> <b>[Oct. 2022]</b> Our paper of investigating whether charge prediction models learn legal knowledge is accepted in EMNLP 2022 </li>
                <li> <b>[May. 2022]</b> I'm awarded with President Scholarship! It is a great hornor for a Ph.D. student in Peking University. </li>
                <li> <b>[Feb. 2022]</b> Our paper on analyzing Document RE Dataset, DocRED, got accepted by ACL 2022. </li>
                <li> <b>[Apr. 2021]</b> Our paper on analyzing Document RE got accepted by ACL 2022 </li> -->
                <!-- <div id="old_news" style="display: none;"> -->
                
                <div>
            </ul>
    </td></tr></tbody>
</table>


<!-- publication -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Publications (Selected)</sectionheading></td></tr></tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody><tr>
    
    <td width="5%" valign="top" align="center">

    </td>

    <td width="90%" valign="top">
        <p>

            <heading> Harder Tasks Need More Experts: Dynamic Routing in MoE Models (ACL 2024)</heading>
            <br>
            <b>Quzhe Huang*</b>, Zhenwei An*, Nan Zhuang*, Mingxu Tao, Chen Zhang, Yang Jin, Kun Xu, Liwei Chen, Songfang Huang, Yansong Feng
            <br> 
            <br>
            <b>TL:DR:</b> We present a dynamic routing mechanism for Mixture of Experts(MoE) models that improves efficiency and performance by adjusting the number of activated experts based on input difficulty. 
            <br>
            | <a href="https://arxiv.org/pdf/2403.07652">paper</a> | <a href="https://github.com/ZhenweiAn/Dynamic_MoE">code</a> |
        </p>

        <!-- <div class="paper"> 
            | <a href="https://arxiv.org/pdf/2403.07652">paper</a> | <a href="https://github.com/ZhenweiAn/Dynamic_MoE"> code & data </a> |
            </pre>
        </div> -->
    </td>

</tr></tbody></table>

<!-- TODO -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody><tr>
    
    <td width="5%" valign="top" align="center">

    </td>

    <td width="90%" valign="top">
        <p>

            <heading> Lawyer LLaMA: Enhancing LLMs with Legal Knowledge (Arxiv)</heading>
            <br>
            <b>Quzhe Huang</b>, Mingxu Tao, Zhenwei An, Chen Zhang, Cong Jiang, Zhibin Chen, Zirui Wu and Yansong Feng 
            <br>
            <br>
            <b>TL:DR:</b> We have designed a framework to adapt a general LLM into a specific domain and built a legal model, Lawyer LLaMA, based on the framework.
            <br>
            | <a href="https://arxiv.org/pdf/2305.15062.pdf">paper</a> | <a href="https://github.com/AndrewZhe/lawyer-llama"> code & data </a> |
        </p>
    </td>
</tr></tbody></table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody><tr>
    
    <td width="5%" valign="top" align="center">

    </td>

    <td width="90%" valign="top">
        <p>

            <heading> Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization (ICML 2024)</heading>
            <br>
            Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, <b>Quzhe Huang</b>, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, Kun Gai, Yadong Mu
            <br> 
            <br>
            <b>TL:DR:</b> Our proposed framework efficiently decomposes videos into keyframes and temporal motions, enabling unified generative pre-training for videos, images, and text.
            <br>
            | <a href="https://arxiv.org/pdf/2402.03161">paper</a> | <a href="https://github.com/jy0205/LaVIT"> code </a> | <a href="https://video-lavit.github.io/">website</a> |
        </p>

    </td>

</tr></tbody></table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody><tr>
    
    <td width="5%" valign="top" align="center">

    </td>

    <td width="90%" valign="top">
        <p>

            <heading> LaVIT: Unified Language-Vision Pretraining with Dynamic Discrete Visual Tokenization (ICLR 2024)</heading>
            <br>
            Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, <b>Quzhe Huang</b>, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, Di Zhang, Wenwu Ou, Kun Gai, Yadong Mu
            <br> 
            <br>
            <b>TL:DR:</b> Our method introduces a visual tokenizer that translates images into discrete tokens, allowing LaVIT to handle and generate multi-modal content seamlessly within a unified generative learning paradigm.
            <br>
            | <a href="https://arxiv.org/pdf/2309.04669">paper</a> | <a href="https://github.com/jy0205/LaVIT"> code </a>|
        </p>

    </td>

</tr></tbody></table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody><tr>
    
    <td width="5%" valign="top" align="center">

    </td>

    <td width="90%" valign="top">
        <p>

            <heading> Probing Multimodal Large Language Models for Global and Local Semantic Representations (COLING 2024)</heading>
            <br>
            Mingxu Tao, <b>Quzhe Huang</b>, Kun Xu, Liwei Chen, Yansong Feng, Dongyan Zhao
            <br> 
            <br>
            <b>TL:DR:</b> The topmost layers of multimodal LLMs may excessively focus on local information, leading to a diminished ability to encode global information.
            <br>
            | <a href="https://arxiv.org/pdf/2402.17304">paper</a> | <a href="https://github.com/kobayashikanna01/probing_MLLM_rep"> code </a>|
        </p>

    </td>

</tr></tbody></table>



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody><tr>
    
    <td width="5%" valign="top" align="center">

    </td>

    <td width="90%" valign="top">
        <p>

            <heading> More than Classification: A Unified Framework for Event Temporal Relation Extraction (ACL 23)</heading>
            <br>

            <b>Quzhe Huang</b>, Yutong Hu, Shengqi Zhu, Yansong Feng, Chang Liu, Dongyan Zhao <br> <br>
            <b>TL:DR:</b> Determining the temporal relation between events based on their start and end time points, instead of treating this problem as a simple classification task.
            <br> | <a href="https://arxiv.org/pdf/2305.17607.pdf">paper</a> | 
        </p>
    </td>

</tr></tbody></table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody><tr>
    
    <td width="5%" valign="top" align="center">

    </td>

    <td width="90%" valign="top">
        <p>

            <heading>Do Charge Prediction Models Learn Legal Theory? (Findings of EMNLP 22)</heading>
            <br>

            Zhenwei An*, <b>Quzhe Huang*</b>, Cong Jiang, Yansong Feng, Dongyan Zhao <br> <br>
            <b>TL:DR:</b> An empirical study of whether charge prediction models make judgments corresponds to the decision logic of human judges.
            <br> | <a href="https://arxiv.org/pdf/2210.17108.pdf">paper</a> |
        </p>

    </td>

</tr></tbody></table>

<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody><tr>
    
    <td width="5%" valign="top" align="center">

    </td>

    <td width="90%" valign="top">
        <p>

            <heading>Rethinking Task-Specific Knowledge Distillation: Contextualized Corpu (EMNLP 22)</heading>
            <br>
            

            Chang Liu, Chongyang Tao, Jianxin Liang, Tao Shen, Jiazhan Feng, <b>Quzhe Huang</b>, Dongyan Zhao <br>
            
        </p>

        <div class="paper"> 
            | <a href="https://aclanthology.org/2022.emnlp-main.729.pdf">paper</a> |
            </pre>
        </div>
    </td>

</tr></tbody></table> -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody><tr>
    
    <td width="5%" valign="top" align="center">

    </td>

    <td width="90%" valign="top">
        <p>

            <heading>Does Recommend-Revise Produce Reliable Annotations? An Analysis on Missing Instances in DocRED (ACL 22)</heading>
            <br>
            

            <b>Quzhe Huang</b>, Shibo Hao, Yuan Ye, Shengqi Zhu, Yansong Feng, Dongyan Zhao <br> <br>
            <b>TL:DR:</b> An analysis of recommend-revise annotation scheme, where existing models or knowledge bases are used to recommend candidate instances and annotators revise the recommendations. We find that in the revision stage, annotators cannot supplement adequate missing instances, which might cause bias in the constructed dataset and the models trained on the new data
            <br>
            | <a href="https://arxiv.org/pdf/2204.07980.pdf">paper</a> | <a href="https://github.com/AndrewZhe/Revisit-DocRED"> code & data </a> |
        </p>

    </td>

</tr></tbody></table>

<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody><tr>
    
    <td width="5%" valign="top" align="center">

    </td>

    <td width="90%" valign="top">
        <p>

            <heading>Knowledge-enhanced Iterative Instruction Generation and Reasoning for Knowledge Base Question Answering (NLPCC 22)</heading>
            <br>
            

            Haowei Du, <b>Quzhe Huang</b>, Chen Zhang, Dongyan Zhao <br>
        </p>

        <div class="paper"> 
            | <a href="https://arxiv.org/pdf/2209.03005.pdf">paper</a> |
            </pre>
        </div>
    </td>

</tr></tbody></table> -->
     

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
<tr onmouseout="rfaction_stop()" onmouseover="rfaction_start()">

    <td width="5%" valign="top" align="center">

    </td>

    <td width="90%" valign="top">
        <p>
            
            <heading>Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction (ACL 21) </heading>
            <br>

            <b>Quzhe Huang</b>, Shengqi Zhu, Yansong Feng, Yuan Ye, Yuxuan Lai, Dongyan Zhao <br> <br>
            <b>TL:DR:</b> Empirically showed that determining the relation between entities in long texts only requires limited evidence, and proposed a method based on co-reference and multi-hop reasoning to select evidence sentences for document-level RE.
            <br> | <a href="https://aclanthology.org/2021.acl-short.126.pdf">paper</a> | <a href="https://github.com/AndrewZhe/Three-Sentences-Are-All-You-Need"> code </a> |
        </p>
    </td>

</tr>
</tbody></table>

<!--  -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody><tr>
    <td width="5%" valign="top" align="center">

    </td>

    <td width="90%" valign="top">
        <p>

            <heading>Exploring Distantly-Labeled Rationales in Neural Network Models (ACL 21)</heading>
            <br>

            <b>Quzhe Huang</b>, Shengqi Zhu, Yansong Feng, Dongyan Zhao <br> <br>
            <b>TL:DR:</b> Proposed a method to prevent the model from ignoring the potential important non-rationale words and not
            distinguishing the importance of different rationale words, when incorporating human rational.

            <br> | <a href="https://aclanthology.org/2021.acl-long.433.pdf">paper</a> |

        </p>
    </td>

</tr></tbody></table>



<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody><tr>
    <td width="5%" valign="top" align="center">

    </td>
    <td width="90%" valign="top">
        <p>

            <heading>Why Machine Reading Comprehension Models Learn Shortcuts? (Findings of ACL 21)</heading>
            <br>

            Yuxuan Lai, Chen Zhang, Yansong Feng, <b>Quzhe Huang</b>, Dongyan Zhao <br> <br>
            An Investigation of the reason why machine reading comprehension models learn shortcuts. We find that it is due to
            the larger proportion of shortcut questions in training data.
        </p>

        <div class="paper"> 
            | <a href="https://aclanthology.org/2021.findings-acl.85.pdf">paper</a> |
            </pre>
        </div>
    </td>

</tr></tbody></table> -->


<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody><tr>

    <td width="5%" valign="top" align="center">

    </td>
    <td width="90%" valign="top">
        <p>

            <heading>Towards Context-Aware Code Comment Generation (Findings of EMNLP 20)</heading>
            <br>

            Xiaohan Yu, <b>Quzhe Huang</b>, Zheng Wang, Yansong Feng, Dongyan Zhao <br>
            
        </p>

        <div class="paper"> 
            | <a href="https://aclanthology.org/2020.findings-emnlp.350.pdf">paper</a> |
            </pre>
        </div>
    </td>

</tr></tbody></table> -->





<!-- service -->

<table width="100%" align="center" border="0" cellpadding="10">
    <tbody><tr><td><sectionheading>&nbsp;&nbsp;Service</sectionheading>
      <ul>
          <li><b>ACL Rollings</b>, area chair, 2023.10</li>
          <li><b>ACL Rollings</b>, reviewer, since 2021.9</li>
          <li><b>ACL</b>, reviewer, 2022,2023</li>
          <li><b>EMNLP</b>, reviewer, 2022,2023</li>
          <li><b>COLING</b>, reviewer, 2022,2023</li>
          <li><b>EACL</b>, reviewer, 2023</li>
          <li><b>AAAI</b>, reviewer, 2023</li>
      </ul>
    </td></tr></tbody>
  </table>


<!-- award -->

<table width="100%" align="center" border="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Awards</sectionheading>
    <ul>
        <li><b>President Scholarship</b>, Peking University, 2022</li>
        <li><b>Uniqlo Scholarship</b>, Peking University, 2017</li>
        <li><b>Panasonic Scholarship</b>, Peking University, 2016</li>
    </ul>
  </td></tr></tbody>
</table>

<!-- contact -->

<table width="100%" align="center" border="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Contact</sectionheading>
        <p style="margin-left: 5%;">
        Wangxuan Institute of Computer Technology, Peking University<br> 
        No. 128 Zhongguancun North Street, <br>
        Haidian District, Beijing, 100871 <br> 
        huangquzhe[AT]pku[DOT]edu[DOT]cn
        </p>
  </td></tr></tbody>
</table>



<!-- footer -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody><tr><td><br>
    <p align="right"><font size="2">
        <b>Website design</b>: <a href="http://www.cs.berkeley.edu/~barron/">&#10025;</a> <a href="https://people.eecs.berkeley.edu/~pathak/">&#10025;</a><br>
    </font></p>
</td></tr></tbody></table>

</td></tr></tbody></table>

</body>
</html>
